{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras \n",
    "from keras.models import Sequential \n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Conv2D, MaxPooling2D, Input, Merge, Activation, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import shuffle\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/xtrain_obfuscated.txt\", \"r\") as f:\n",
    "    data_train_x = f.read()\n",
    "data_train_x = data_train_x.split(\"\\n\")\n",
    "\n",
    "with open(\"data/ytrain.txt\", \"r\") as f:\n",
    "    data_train_y = f.read()\n",
    "data_train_y = data_train_y.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_x = data_train_x[:-1]\n",
    "data_train_y = data_train_y[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(i) for i in data_train_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_width = max(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad short sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_x_norm = []\n",
    "\n",
    "for i in data_train_x:\n",
    "    padding_length = input_width - len(i)\n",
    "    padding = \"\".join([\"w\"] * padding_length)\n",
    "    padded = i + padding\n",
    "    data_train_x_norm.append(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_eval, y_train, y_eval = train_test_split(data_train_x_norm, data_train_y, \n",
    "                                                    test_size=0.1, random_state=2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(x_train))\n",
    "print (len(x_eval))\n",
    "print (len(y_train))\n",
    "print (len(y_eval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get reference characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique characters\n",
    "characters = list(set([i for text in data_train_x_norm for i in text]))\n",
    "characters.sort()\n",
    "characters = np.array(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_classes=12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(input_width=452, input_height=26, number_of_classes=12, channel = 1):  \n",
    "    #variable initialization \n",
    "    nb_filters =32\n",
    "    kernel_size= {}\n",
    "    kernel_size[0]= 3\n",
    "    kernel_size[1]= 4\n",
    "    kernel_size[2]= 5\n",
    "    input_shape=(452, 26, 1)\n",
    "    pool_size = (5,23)\n",
    "    nb_classes =12\n",
    "    no_parallel_filters = 3  \n",
    "    \n",
    "    inp = Input(shape=(input_width, input_height))\n",
    "\n",
    "    # create seperate model graph for parallel processing with different filter sizes\n",
    "    # apply 'same' padding so that ll produce o/p tensor of same size for concatination\n",
    "    # cancat all paralle output\n",
    "\n",
    "    convs = []\n",
    "    for k_no in range(len(kernel_size)):\n",
    "        conv = Conv1D(nb_filters, kernel_size[k_no],\n",
    "                        border_mode='same',\n",
    "                             activation='relu',\n",
    "                        input_shape= input_shape)(inp)\n",
    "        convs.append(conv)\n",
    "\n",
    "    if len(kernel_size) > 1:\n",
    "        out = Merge(mode='concat')(convs)\n",
    "    else:\n",
    "        out = convs[0]\n",
    "\n",
    "    conv_model = Model(input=inp, output=out)\n",
    "\n",
    "    # add created model graph in sequential model\n",
    "    model = Sequential()\n",
    "    model.add(conv_model)        # add model just like layer\n",
    "    model.add(Conv1D(nb_filters, kernel_size[1]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=5))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten(input_shape=input_shape))\n",
    "    model.add(Dense(nb_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model5 has the highest performance \n",
    "model = createModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeInputText(text, characters):\n",
    "    \"\"\"  \"\"\"\n",
    "    encoded_text = []\n",
    "    for char in text:\n",
    "        char_vector = np.zeros(characters.shape[0])\n",
    "        char_vector[characters == char] = 1\n",
    "        encoded_text.append(char_vector)\n",
    "\n",
    "    encoded_text = np.array(encoded_text)\n",
    "    return encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeInputLabel(raw_label, number_of_classes):\n",
    "    \"\"\" convert class id to one hot vector \"\"\"\n",
    "    encoded_label = np.zeros(number_of_classes)\n",
    "    encoded_label[int(raw_label)] = 1.0\n",
    "\n",
    "    return encoded_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateBatch(x_train, y_train, characters, number_of_classes, batch_size=64):\n",
    "    i = 0\n",
    "    while i < len(x_train):\n",
    "        # get batch data\n",
    "        x_batch_raw = x_train[i : i + batch_size]\n",
    "        y_batch_raw = y_train[i : i + batch_size]\n",
    "        \n",
    "        # encode to numpy tensor\n",
    "        x_batch = np.array([encodeInputText(text, characters) for text in x_batch_raw])\n",
    "        y_batch = np.array([encodeInputLabel(label, number_of_classes) for label in y_batch_raw]) \n",
    "        \n",
    "        i += batch_size\n",
    "        \n",
    "        yield x_batch, y_batch\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    print (\"+++++++++++++++++++++++++++++++++++++\")\n",
    "    print (\"Epoch: {}\".format(epoch))\n",
    "    \n",
    "    # shuffle data at the beginning of each epoch\n",
    "    indices = [i for i in range(len(x_train))]\n",
    "    shuffle(indices)\n",
    "    x_train = [x_train[sample_index] for sample_index in indices]\n",
    "    y_train = [y_train[sample_index] for sample_index in indices]\n",
    "    \n",
    "    # train for single epoch\n",
    "    print (\"Training ...\")\n",
    "    for batch_count, (x_batch, y_batch) in enumerate(generateBatch(x_train, y_train, characters, number_of_classes)):\n",
    "        sys.stdout.write(\"\\rFinished {} batches\".format(batch_count))\n",
    "        sys.stdout.flush()\n",
    "        model.train_on_batch(x=x_batch, y=y_batch)\n",
    "    \n",
    "    # evaluate on eval\n",
    "    print (\"Evaluating on eval set\")\n",
    "    x_eval_enc = np.array([encodeInputText(text, characters) for text in x_eval])\n",
    "    y_eval_enc = np.array([encodeInputLabel(label, number_of_classes) for label in y_eval])\n",
    "    \n",
    "    results = model.evaluate(x_eval_enc, y_eval_enc)\n",
    "    print (\"\")\n",
    "    print (\"Eval set loss: {}\".format(results[0]))\n",
    "    print (\"Eval set accuracy: {}\".format(results[1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "with open(\"xtest_obfuscated.txt\", \"r\") as f:\n",
    "    data_test = f.read()\n",
    "data_test = data_test.split(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty line at the end\n",
    "data_test = data_test[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad short text \n",
    "data_test_norm = []\n",
    "for i in data_test:\n",
    "    padding_length = input_width - len(i)\n",
    "    padding = \"\".join([\"w\"] * padding_length)\n",
    "    padded = i + padding\n",
    "    data_test_norm.append(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode text into numpy array\n",
    "x_test = np.array([encodeInputText(text, characters) for text in data_test_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the indices of the max score for each row\n",
    "novel_ids = predictions.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novel_ids[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'y_test.txt' \n",
    "def save_labels(file_name,novel_ids):  \n",
    "    labels = open(file_name, 'w') \n",
    "    for item in novel_ids: \n",
    "      labels.write(\"%s\\n\" % item) \n",
    "save_labels(file_name, novel_ids) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
